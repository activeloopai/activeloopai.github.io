<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>hub.core.storage.lru_cache API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>hub.core.storage.lru_cache</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from collections import OrderedDict
from hub.core.storage.provider import StorageProvider
from typing import Set

# TODO use lock for multiprocessing
class LRUCache(StorageProvider):
    &#34;&#34;&#34;LRU Cache that uses StorageProvider for caching&#34;&#34;&#34;

    def __init__(
        self,
        cache_storage: StorageProvider,
        next_storage: StorageProvider,
        cache_size: int,
    ):
        &#34;&#34;&#34;Initializes the LRUCache. It can be chained with other LRUCache objects to create multilayer caches.

        Args:
            cache_storage (StorageProvider): The storage being used as the caching layer of the cache.
                This should be a base provider such as MemoryProvider, LocalProvider or S3Provider but not another LRUCache.
            next_storage (StorageProvider): The next storage layer of the cache.
                This can either be a base provider (i.e. it is the final storage) or another LRUCache (i.e. in case of chained cache).
                While reading data, all misses from cache would be retrieved from here.
                While writing data, the data will be written to the next_storage when cache_storage is full or flush is called.
            cache_size (int): The total space that can be used from the cache_storage in bytes.
                This number may be less than the actual space available on the cache_storage.
                Setting it to a higher value than actually available space may lead to unexpected behaviors.
        &#34;&#34;&#34;
        self.next_storage = next_storage
        self.cache_storage = cache_storage
        self.cache_size = cache_size

        # tracks keys in lru order, stores size of value, only keys present in this exist in cache
        self.lru_sizes: OrderedDict[str, int] = OrderedDict()
        self.dirty_keys: Set[str] = set()  # keys present in cache but not next_storage
        self.cache_used = 0

    def flush(self):
        &#34;&#34;&#34;Writes data from cache_storage to next_storage. Only the dirty keys are written.
        This is a cascading function and leads to data being written to the final storage in case of a chained cache.
        &#34;&#34;&#34;
        for key in self.dirty_keys:
            self.next_storage[key] = self.cache_storage[key]
        self.dirty_keys.clear()

        self.next_storage.flush()

    def __getitem__(self, path: str):
        &#34;&#34;&#34;If item is in cache_storage, retrieves from there and returns.
        If item isn&#39;t in cache_storage, retrieves from next storage, stores in cache_storage (if possible) and returns.

        Args:
            path (str): the path relative to the root of the underlying storage.

        Raises:
            KeyError: if an object is not found at the path.

        Returns:
            bytes: The bytes of the object present at the path.
        &#34;&#34;&#34;
        if path in self.lru_sizes:
            self.lru_sizes.move_to_end(path)  # refresh position for LRU
            return self.cache_storage[path]
        else:
            result = self.next_storage[path]  # fetch from storage, may throw KeyError
            if len(result) &lt;= self.cache_size:  # insert in cache if it fits
                self._insert_in_cache(path, result)
            return result

    def __setitem__(self, path: str, value: bytes):
        &#34;&#34;&#34;Puts the item in the cache_storage (if possible), else writes to next_storage.

        Args:
            path (str): the path relative to the root of the underlying storage.
            value (bytes): the value to be assigned at the path.
        &#34;&#34;&#34;
        if path in self.lru_sizes:
            size = self.lru_sizes.pop(path)
            self.cache_used -= size

        if len(value) &lt;= self.cache_size:
            self._insert_in_cache(path, value)
            self.dirty_keys.add(path)
        else:  # larger than cache, directly send to next layer
            self.dirty_keys.discard(path)
            self.next_storage[path] = value

    def __delitem__(self, path: str):
        &#34;&#34;&#34;Deletes the object present at the path from the cache and the underlying storage.

        Args:
            path (str): the path to the object relative to the root of the provider.

        Raises:
            KeyError: If an object is not found at the path.
        &#34;&#34;&#34;
        deleted_from_cache = False
        if path in self.lru_sizes:
            size = self.lru_sizes.pop(path)
            self.cache_used -= size
            del self.cache_storage[path]
            self.dirty_keys.discard(path)
            deleted_from_cache = True

        try:
            del self.next_storage[path]
        except KeyError:
            if not deleted_from_cache:
                raise

    def __len__(self):
        &#34;&#34;&#34;Returns the number of files present in the cache and the underlying storage.

        Returns:
            int: the number of files present inside the root.
        &#34;&#34;&#34;
        return len(self._list_keys())

    def __iter__(self):
        &#34;&#34;&#34;Generator function that iterates over the keys of the cache and the underlying storage.

        Yields:
            str: the path of the object that it is iterating over, relative to the root of the provider.
        &#34;&#34;&#34;
        yield from self._list_keys()

    def _free_up_space(self, extra_size: int):
        &#34;&#34;&#34;Helper function that frees up space the requred space in cache.
            No action is taken if there is sufficient space in the cache.

        Args:
            extra_size (int): the space that needs is required in bytes.
        &#34;&#34;&#34;
        while self.cache_used &gt; 0 and extra_size + self.cache_used &gt; self.cache_size:
            self._pop_from_cache()

    def _pop_from_cache(self):
        &#34;&#34;&#34;Helper function that pops the least recently used key, value pair from the cache&#34;&#34;&#34;
        key, itemsize = self.lru_sizes.popitem(last=False)
        if key in self.dirty_keys:
            self.next_storage[key] = self.cache_storage[key]
            self.dirty_keys.discard(key)
        del self.cache_storage[key]
        self.cache_used -= itemsize

    def _insert_in_cache(self, path: str, value: bytes):
        &#34;&#34;&#34;Helper function that adds a key value pair to the cache.

        Args:
            path (str): the path relative to the root of the underlying storage.
            value (bytes): the value to be assigned at the path.
        &#34;&#34;&#34;
        self._free_up_space(len(value))
        self.cache_storage[path] = value
        self.cache_used += len(value)
        self.lru_sizes[path] = len(value)

    def _list_keys(self):
        &#34;&#34;&#34;Helper function that lists all the objects present in the cache and the underlying storage.

        Returns:
            list: list of all the objects found in the cache and the underlying storage.
        &#34;&#34;&#34;
        all_keys = {key for key in self.next_storage}
        for key in self.cache_storage:
            all_keys.add(key)
        return list(all_keys)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="hub.core.storage.lru_cache.LRUCache"><code class="flex name class">
<span>class <span class="ident">LRUCache</span></span>
<span>(</span><span>cache_storage: <a title="hub.core.storage.provider.StorageProvider" href="provider.html#hub.core.storage.provider.StorageProvider">StorageProvider</a>, next_storage: <a title="hub.core.storage.provider.StorageProvider" href="provider.html#hub.core.storage.provider.StorageProvider">StorageProvider</a>, cache_size: int)</span>
</code></dt>
<dd>
<div class="desc"><p>LRU Cache that uses StorageProvider for caching</p>
<p>Initializes the LRUCache. It can be chained with other LRUCache objects to create multilayer caches.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>cache_storage</code></strong> :&ensp;<code>StorageProvider</code></dt>
<dd>The storage being used as the caching layer of the cache.
This should be a base provider such as MemoryProvider, LocalProvider or S3Provider but not another LRUCache.</dd>
<dt><strong><code>next_storage</code></strong> :&ensp;<code>StorageProvider</code></dt>
<dd>The next storage layer of the cache.
This can either be a base provider (i.e. it is the final storage) or another LRUCache (i.e. in case of chained cache).
While reading data, all misses from cache would be retrieved from here.
While writing data, the data will be written to the next_storage when cache_storage is full or flush is called.</dd>
<dt><strong><code>cache_size</code></strong> :&ensp;<code>int</code></dt>
<dd>The total space that can be used from the cache_storage in bytes.
This number may be less than the actual space available on the cache_storage.
Setting it to a higher value than actually available space may lead to unexpected behaviors.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LRUCache(StorageProvider):
    &#34;&#34;&#34;LRU Cache that uses StorageProvider for caching&#34;&#34;&#34;

    def __init__(
        self,
        cache_storage: StorageProvider,
        next_storage: StorageProvider,
        cache_size: int,
    ):
        &#34;&#34;&#34;Initializes the LRUCache. It can be chained with other LRUCache objects to create multilayer caches.

        Args:
            cache_storage (StorageProvider): The storage being used as the caching layer of the cache.
                This should be a base provider such as MemoryProvider, LocalProvider or S3Provider but not another LRUCache.
            next_storage (StorageProvider): The next storage layer of the cache.
                This can either be a base provider (i.e. it is the final storage) or another LRUCache (i.e. in case of chained cache).
                While reading data, all misses from cache would be retrieved from here.
                While writing data, the data will be written to the next_storage when cache_storage is full or flush is called.
            cache_size (int): The total space that can be used from the cache_storage in bytes.
                This number may be less than the actual space available on the cache_storage.
                Setting it to a higher value than actually available space may lead to unexpected behaviors.
        &#34;&#34;&#34;
        self.next_storage = next_storage
        self.cache_storage = cache_storage
        self.cache_size = cache_size

        # tracks keys in lru order, stores size of value, only keys present in this exist in cache
        self.lru_sizes: OrderedDict[str, int] = OrderedDict()
        self.dirty_keys: Set[str] = set()  # keys present in cache but not next_storage
        self.cache_used = 0

    def flush(self):
        &#34;&#34;&#34;Writes data from cache_storage to next_storage. Only the dirty keys are written.
        This is a cascading function and leads to data being written to the final storage in case of a chained cache.
        &#34;&#34;&#34;
        for key in self.dirty_keys:
            self.next_storage[key] = self.cache_storage[key]
        self.dirty_keys.clear()

        self.next_storage.flush()

    def __getitem__(self, path: str):
        &#34;&#34;&#34;If item is in cache_storage, retrieves from there and returns.
        If item isn&#39;t in cache_storage, retrieves from next storage, stores in cache_storage (if possible) and returns.

        Args:
            path (str): the path relative to the root of the underlying storage.

        Raises:
            KeyError: if an object is not found at the path.

        Returns:
            bytes: The bytes of the object present at the path.
        &#34;&#34;&#34;
        if path in self.lru_sizes:
            self.lru_sizes.move_to_end(path)  # refresh position for LRU
            return self.cache_storage[path]
        else:
            result = self.next_storage[path]  # fetch from storage, may throw KeyError
            if len(result) &lt;= self.cache_size:  # insert in cache if it fits
                self._insert_in_cache(path, result)
            return result

    def __setitem__(self, path: str, value: bytes):
        &#34;&#34;&#34;Puts the item in the cache_storage (if possible), else writes to next_storage.

        Args:
            path (str): the path relative to the root of the underlying storage.
            value (bytes): the value to be assigned at the path.
        &#34;&#34;&#34;
        if path in self.lru_sizes:
            size = self.lru_sizes.pop(path)
            self.cache_used -= size

        if len(value) &lt;= self.cache_size:
            self._insert_in_cache(path, value)
            self.dirty_keys.add(path)
        else:  # larger than cache, directly send to next layer
            self.dirty_keys.discard(path)
            self.next_storage[path] = value

    def __delitem__(self, path: str):
        &#34;&#34;&#34;Deletes the object present at the path from the cache and the underlying storage.

        Args:
            path (str): the path to the object relative to the root of the provider.

        Raises:
            KeyError: If an object is not found at the path.
        &#34;&#34;&#34;
        deleted_from_cache = False
        if path in self.lru_sizes:
            size = self.lru_sizes.pop(path)
            self.cache_used -= size
            del self.cache_storage[path]
            self.dirty_keys.discard(path)
            deleted_from_cache = True

        try:
            del self.next_storage[path]
        except KeyError:
            if not deleted_from_cache:
                raise

    def __len__(self):
        &#34;&#34;&#34;Returns the number of files present in the cache and the underlying storage.

        Returns:
            int: the number of files present inside the root.
        &#34;&#34;&#34;
        return len(self._list_keys())

    def __iter__(self):
        &#34;&#34;&#34;Generator function that iterates over the keys of the cache and the underlying storage.

        Yields:
            str: the path of the object that it is iterating over, relative to the root of the provider.
        &#34;&#34;&#34;
        yield from self._list_keys()

    def _free_up_space(self, extra_size: int):
        &#34;&#34;&#34;Helper function that frees up space the requred space in cache.
            No action is taken if there is sufficient space in the cache.

        Args:
            extra_size (int): the space that needs is required in bytes.
        &#34;&#34;&#34;
        while self.cache_used &gt; 0 and extra_size + self.cache_used &gt; self.cache_size:
            self._pop_from_cache()

    def _pop_from_cache(self):
        &#34;&#34;&#34;Helper function that pops the least recently used key, value pair from the cache&#34;&#34;&#34;
        key, itemsize = self.lru_sizes.popitem(last=False)
        if key in self.dirty_keys:
            self.next_storage[key] = self.cache_storage[key]
            self.dirty_keys.discard(key)
        del self.cache_storage[key]
        self.cache_used -= itemsize

    def _insert_in_cache(self, path: str, value: bytes):
        &#34;&#34;&#34;Helper function that adds a key value pair to the cache.

        Args:
            path (str): the path relative to the root of the underlying storage.
            value (bytes): the value to be assigned at the path.
        &#34;&#34;&#34;
        self._free_up_space(len(value))
        self.cache_storage[path] = value
        self.cache_used += len(value)
        self.lru_sizes[path] = len(value)

    def _list_keys(self):
        &#34;&#34;&#34;Helper function that lists all the objects present in the cache and the underlying storage.

        Returns:
            list: list of all the objects found in the cache and the underlying storage.
        &#34;&#34;&#34;
        all_keys = {key for key in self.next_storage}
        for key in self.cache_storage:
            all_keys.add(key)
        return list(all_keys)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="hub.core.storage.provider.StorageProvider" href="provider.html#hub.core.storage.provider.StorageProvider">StorageProvider</a></li>
<li>abc.ABC</li>
<li>collections.abc.MutableMapping</li>
<li>collections.abc.Mapping</li>
<li>collections.abc.Collection</li>
<li>collections.abc.Sized</li>
<li>collections.abc.Iterable</li>
<li>collections.abc.Container</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="hub.core.storage.lru_cache.LRUCache.flush"><code class="name flex">
<span>def <span class="ident">flush</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Writes data from cache_storage to next_storage. Only the dirty keys are written.
This is a cascading function and leads to data being written to the final storage in case of a chained cache.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flush(self):
    &#34;&#34;&#34;Writes data from cache_storage to next_storage. Only the dirty keys are written.
    This is a cascading function and leads to data being written to the final storage in case of a chained cache.
    &#34;&#34;&#34;
    for key in self.dirty_keys:
        self.next_storage[key] = self.cache_storage[key]
    self.dirty_keys.clear()

    self.next_storage.flush()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="hub.core.storage.provider.StorageProvider" href="provider.html#hub.core.storage.provider.StorageProvider">StorageProvider</a></b></code>:
<ul class="hlist">
<li><code><a title="hub.core.storage.provider.StorageProvider.get_bytes" href="provider.html#hub.core.storage.provider.StorageProvider.get_bytes">get_bytes</a></code></li>
<li><code><a title="hub.core.storage.provider.StorageProvider.set_bytes" href="provider.html#hub.core.storage.provider.StorageProvider.set_bytes">set_bytes</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="hub.core.storage" href="index.html">hub.core.storage</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="hub.core.storage.lru_cache.LRUCache" href="#hub.core.storage.lru_cache.LRUCache">LRUCache</a></code></h4>
<ul class="">
<li><code><a title="hub.core.storage.lru_cache.LRUCache.flush" href="#hub.core.storage.lru_cache.LRUCache.flush">flush</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>